{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FD2425\n",
    "Jorge Machado 21181\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import tkinter as tk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVR, NuSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, median_absolute_error,r2_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy Predictor Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with the specified columns\n",
    "df = pd.read_csv('./cars.csv')\n",
    "\n",
    "# Remove rows with NaN in any column\n",
    "df = df.dropna().copy()\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Sample the data (e.g., 10% of the original data)\n",
    "df_sample = df.sample(frac=0.5, random_state=50)\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = df_sample.drop(['price_usd'], axis=1)\n",
    "y = np.log1p(df_sample['price_usd'])  # Use log-transformed target for training\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize LazyRegressor\n",
    "reg = LazyRegressor()\n",
    "\n",
    "# Fit and evaluate models\n",
    "models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Display all the evaluated models\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "Regression = learn f with continuous output values -> car value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min price: 1.0\n",
      "Max price: 50000.0\n",
      "Mean price: 6637.160884193038\n",
      "Standard deviation of price: 6425.19899630064\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with the specified columns\n",
    "df = pd.read_csv('./cars.csv')\n",
    "\n",
    "\"\"\" df = pd.read_csv('./cars.csv', usecols=['manufacturer_name', 'model_name', 'transmission', 'color', \n",
    "                                        'odometer_value', 'year_produced', 'engine_fuel', 'engine_has_gas', \n",
    "                                        'engine_type', 'engine_capacity', 'body_type', 'has_warranty', \n",
    "                                        'state', 'drivetrain', 'price_usd', 'is_exchangeable']) \"\"\"\n",
    "\n",
    "# Remove rows with NaN in any column\n",
    "df = df.dropna().copy()\n",
    "\n",
    "df.head()\n",
    "\n",
    "print(\"Min price:\", df['price_usd'].min())\n",
    "print(\"Max price:\", df['price_usd'].max())\n",
    "print(\"Mean price:\", df['price_usd'].mean())\n",
    "print(\"Standard deviation of price:\", df['price_usd'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       odometer_value  year_produced  engine_capacity  price_usd  \\\n",
      "count        38521.00       38521.00         38521.00   38521.00   \n",
      "mean        248910.07        2002.94             2.06    6637.16   \n",
      "std         136059.50           8.06             0.67    6425.20   \n",
      "min              0.00        1942.00             0.20       1.00   \n",
      "25%         158000.00        1998.00             1.60    2100.00   \n",
      "50%         250000.00        2003.00             2.00    4800.00   \n",
      "75%         325000.00        2009.00             2.30    8950.00   \n",
      "max        1000000.00        2019.00             8.00   50000.00   \n",
      "\n",
      "       number_of_photos  up_counter  duration_listed  \n",
      "count          38521.00    38521.00         38521.00  \n",
      "mean               9.65       16.31            80.58  \n",
      "std                6.09       43.29           112.84  \n",
      "min                1.00        1.00             0.00  \n",
      "25%                5.00        2.00            23.00  \n",
      "50%                8.00        5.00            59.00  \n",
      "75%               12.00       16.00            91.00  \n",
      "max               86.00     1861.00          2232.00   \n",
      "\n",
      "(38521, 30) \n",
      "\n",
      "\n",
      "Index(['manufacturer_name', 'model_name', 'transmission', 'color',\n",
      "       'odometer_value', 'year_produced', 'engine_fuel', 'engine_has_gas',\n",
      "       'engine_type', 'engine_capacity', 'body_type', 'has_warranty', 'state',\n",
      "       'drivetrain', 'price_usd', 'is_exchangeable', 'location_region',\n",
      "       'number_of_photos', 'up_counter', 'feature_0', 'feature_1', 'feature_2',\n",
      "       'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7',\n",
      "       'feature_8', 'feature_9', 'duration_listed'],\n",
      "      dtype='object') \n",
      "\n",
      "\n",
      "manufacturer_name    0\n",
      "model_name           0\n",
      "transmission         0\n",
      "color                0\n",
      "odometer_value       0\n",
      "year_produced        0\n",
      "engine_fuel          0\n",
      "engine_has_gas       0\n",
      "engine_type          0\n",
      "engine_capacity      0\n",
      "body_type            0\n",
      "has_warranty         0\n",
      "state                0\n",
      "drivetrain           0\n",
      "price_usd            0\n",
      "is_exchangeable      0\n",
      "location_region      0\n",
      "number_of_photos     0\n",
      "up_counter           0\n",
      "feature_0            0\n",
      "feature_1            0\n",
      "feature_2            0\n",
      "feature_3            0\n",
      "feature_4            0\n",
      "feature_5            0\n",
      "feature_6            0\n",
      "feature_7            0\n",
      "feature_8            0\n",
      "feature_9            0\n",
      "duration_listed      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.describe(), \"\\n\")\n",
    "print(df.shape, \"\\n\\n\")\n",
    "print (df.columns, \"\\n\\n\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns\n",
    "numerical_cols = ['price_usd', 'year_produced', 'odometer_value', 'engine_capacity', 'duration_listed', 'number_of_photos']\n",
    "categorical_cols = ['model_name', 'has_warranty', 'is_exchangeable', 'transmission', 'color', 'engine_fuel', 'state', 'drivetrain', 'engine_type', 'body_type', 'engine_has_gas', 'manufacturer_name']\n",
    "\n",
    "# Apply StandardScaler to numerical columns\n",
    "scaler = StandardScaler()\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for modeling\n",
    "X = df[['manufacturer_name', 'model_name', 'year_produced', 'odometer_value', 'color', 'transmission', 'engine_fuel', 'engine_type',\n",
    "        'engine_capacity', 'body_type', 'has_warranty', 'state', 'drivetrain', 'is_exchangeable']]\n",
    "\n",
    "df['log_price_usd'] = np.log1p(df['price_usd'])\n",
    "y = df['log_price_usd']\n",
    "\n",
    "# Encode categorical features\n",
    "X = pd.get_dummies(X, columns=['manufacturer_name', 'model_name', 'year_produced', 'odometer_value', 'color', 'transmission', 'engine_fuel', 'engine_type',\n",
    "                               'engine_capacity', 'body_type', 'has_warranty', 'state', 'drivetrain', 'is_exchangeable'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values found in y_train for LGBMRegressor. Skipping this model.\n",
      "NaN values found in y_train for HistGradientBoostingRegressor. Skipping this model.\n",
      "NaN values found in y_train for GradientBoostingRegressor. Skipping this model.\n"
     ]
    }
   ],
   "source": [
    "# Split the data into 80% training and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Split the 80% training data into 75% training and 25% validation (which is 60% training and 20% validation of the original data)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Replace spaces in feature names with underscores\n",
    "X_train.columns = X_train.columns.str.replace(' ', '_')\n",
    "X_test.columns = X_test.columns.str.replace(' ', '_')\n",
    "x_train.columns = x_train.columns.str.replace(' ', '_')\n",
    "x_val.columns = x_val.columns.str.replace(' ', '_')\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metrics(y_test, y_pred):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # MAPE in percentage\n",
    "    return r2, mae, mse, rmse, mape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1, 4):\n",
    "    if i == 1:\n",
    "        model = LGBMRegressor()\n",
    "        model_name = \"LGBMRegressor\"\n",
    "    elif i == 2:\n",
    "        model = HistGradientBoostingRegressor()\n",
    "        model_name = \"HistGradientBoostingRegressor\"\n",
    "    elif i == 3:\n",
    "        model = GradientBoostingRegressor()\n",
    "        model_name = \"GradientBoostingRegressor\"\n",
    "        \n",
    "    mask = ~np.isnan(y_train)\n",
    "    x_train = x_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "\n",
    "    # Check for NaN values in y_train before fitting\n",
    "    if np.isnan(y_train).any():\n",
    "        print(f\"NaN values found in y_train for {model_name}. Skipping this model.\")\n",
    "        continue\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = np.expm1(model.predict(X_test))\n",
    "\n",
    "    # Clean NaN values from y_test and y_pred\n",
    "    nan_indices = np.isnan(y_test) | np.isnan(y_pred)\n",
    "    y_test_clean = y_test[~nan_indices]\n",
    "    y_pred_clean = y_pred[~nan_indices]\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2, mae, mse, rmse, mape = calculate_metrics(y_test_clean, y_pred_clean)\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X_test.iloc[:, 0], y_pred, c='r', marker='*')  # Use .iloc for DataFrames\n",
    "    ax.set_xlabel(\"idk\")\n",
    "    ax.set_ylabel(\"idk\")\n",
    "    ax.set_title(\"Predicted Values\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Resultado do {model_name}\\n\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAPE: {mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for modeling\n",
    "X = df[['manufacturer_name','model_name', 'year_produced', 'odometer_value', 'color', 'transmission', 'engine_fuel',\n",
    "       'engine_capacity', 'state', 'drivetrain', 'is_exchangeable' ]]\n",
    "\n",
    "y = df['price_usd']  # Target variable\n",
    "\n",
    "# Apply log transformation to the target variable\n",
    "df['log_price_usd'] = np.log1p(df['price_usd'])\n",
    "\n",
    "\n",
    "\n",
    "# Encode categorical features\n",
    "X = pd.get_dummies(X, columns=['manufacturer_name','model_name', 'year_produced', 'odometer_value', 'color', 'transmission', 'engine_fuel',\n",
    "       'engine_capacity', 'state', 'drivetrain', 'is_exchangeable' ], drop_first=True)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets without stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.60, random_state=42)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Replace spaces in feature names with underscores\n",
    "X_train.columns = X_train.columns.str.replace(' ', '_')\n",
    "X_test.columns = X_test.columns.str.replace(' ', '_')\n",
    "x_train.columns = x_train.columns.str.replace(' ', '_')\n",
    "x_val.columns = x_val.columns.str.replace(' ', '_')\n",
    "\n",
    "\n",
    "model = LGBMRegressor()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = np.expm1(model.predict(X_test))  # Convert predictions back to USD\n",
    "\n",
    "\n",
    "# Check for NaN values in y_test and y_pred\n",
    "nan_indices = np.isnan(y_test) | np.isnan(y_pred)\n",
    "\n",
    "# Remove NaN values\n",
    "y_test_clean = y_test[~nan_indices]\n",
    "y_pred_clean = y_pred[~nan_indices]\n",
    "\n",
    "# Calculate r2_score\n",
    "r2 = r2_score(y_test_clean, y_pred_clean)\n",
    "mae = mean_absolute_error(y_test_clean, y_pred_clean)\n",
    "mse = mean_squared_error(y_test_clean, y_pred_clean)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test_clean - y_pred_clean) / y_test_clean)) * 100  # MAPE in percentage\n",
    "\n",
    "print (\"Resultado do LGBMRegressor\\n\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thrid Tests** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().copy()\n",
    "\n",
    "# Feature selection\n",
    "X = df[['manufacturer_name', 'model_name', 'year_produced', 'odometer_value', 'color', 'transmission', 'engine_fuel', \n",
    "        'engine_capacity', 'drivetrain']]\n",
    "\n",
    "# Target variable with log transformation\n",
    "y = np.log1p(df['price_usd'])\n",
    "\n",
    "# Encode categorical features correctly (avoid encoding numeric features)\n",
    "categorical_features = ['manufacturer_name', 'model_name', 'color', 'transmission', 'engine_fuel', 'drivetrain']\n",
    "X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Train-test split (correct order)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.6, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Replace spaces in feature names with underscores\n",
    "X_train.columns = X_train.columns.str.replace(' ', '_')\n",
    "X_test.columns = X_test.columns.str.replace(' ', '_')\n",
    "x_train.columns = x_train.columns.str.replace(' ', '_')\n",
    "x_val.columns = x_val.columns.str.replace(' ', '_')\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "model = LGBMRegressor()\n",
    "\n",
    "# Fit model on training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = np.expm1(model.predict(X_test))  # Convert predictions back to USD\n",
    "\n",
    "# Check for NaN values in y_test and y_pred\n",
    "nan_indices = np.isnan(y_test) | np.isnan(y_pred)\n",
    "\n",
    "# Remove NaN values\n",
    "y_test_clean = y_test[~nan_indices]\n",
    "y_pred_clean = y_pred[~nan_indices]\n",
    "\n",
    "# Calculate r2_score\n",
    "r2 = r2_score(y_test_clean, y_pred_clean)\n",
    "mae = mean_absolute_error(y_test_clean, y_pred_clean)\n",
    "mse = mean_squared_error(y_test_clean, y_pred_clean)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test_clean - y_pred_clean) / y_test_clean)) * 100  # MAPE in percentage\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\\nResultado do LGBMRegressor\\n\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fourth Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for modeling\n",
    "X = df[['manufacturer_name','model_name', 'year_produced', 'odometer_value', 'color' ]]\n",
    "\n",
    "#y = df['price_usd']  # Target variable\n",
    "\n",
    "# Apply log transformation to the target variable\n",
    "y =df['log_price_usd'] = np.log1p(df['price_usd'])\n",
    "\n",
    "# Encode categorical features\n",
    "X = pd.get_dummies(X, columns=['manufacturer_name','model_name', 'year_produced', 'odometer_value', 'color'  ], drop_first=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets without stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.60, random_state=42)\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Replace spaces in feature names with underscores\n",
    "X_train.columns = X_train.columns.str.replace(' ', '_')\n",
    "X_test.columns = X_test.columns.str.replace(' ', '_')\n",
    "x_train.columns = x_train.columns.str.replace(' ', '_')\n",
    "x_val.columns = x_val.columns.str.replace(' ', '_')\n",
    "\n",
    "\n",
    "model = LGBMRegressor()\n",
    "\n",
    "\n",
    "# Fit the model using the correct training split\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate using X_test (testing set)\n",
    "y_pred = np.expm1(model.predict(X_test))  # Convert predictions back to USD\n",
    "\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # MAPE in percentage\n",
    "\n",
    "print (\"\\n\\nResultado do LGBMRegressor\\n\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fith Tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for modeling\n",
    "X = df[['manufacturer_name','model_name', 'year_produced', 'odometer_value', 'color', 'transmission' ]]\n",
    "\n",
    "y = df['price_usd']  # Target variable\n",
    "\n",
    "# Apply log transformation to the target variable\n",
    "y= df['log_price_usd'] = np.log1p(df['price_usd'])\n",
    "\n",
    "# Encode categorical features\n",
    "X = pd.get_dummies(X, columns=['manufacturer_name','model_name', 'year_produced', 'odometer_value', 'color'  ], drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets without stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, train_size=0.60, random_state=42)\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Replace spaces in feature names with underscores\n",
    "X_train.columns = X_train.columns.str.replace(' ', '_')\n",
    "X_test.columns = X_test.columns.str.replace(' ', '_')\n",
    "x_train.columns = x_train.columns.str.replace(' ', '_')\n",
    "x_val.columns = x_val.columns.str.replace(' ', '_')\n",
    "\n",
    "\n",
    "model = LGBMRegressor()\n",
    "\n",
    "\n",
    "# Fit the model using the correct training split\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate using X_test (testing set)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # MAPE in percentage\n",
    "\n",
    "print (\"\\n\\nResultado do LGBMRegressor\\n\")\n",
    "print(f\"R²: {r2:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metricas utilizadas para a regressão\n",
    "\n",
    "# MSE – Mean Squared error [1]\n",
    "Mede a diferença média quadrática entre os valores previstos e os valores reais no conjunto de dados.\n",
    "\n",
    "**Vantagens:**\n",
    "\n",
    "- Fornece uma medida abrangente da precisão do modelo.\n",
    "- Sensível tanto a grandes quanto a pequenos erros.\n",
    "- Fácil de calcular e interpretar.\n",
    "\n",
    "**Limitações:**\n",
    "\n",
    "- Pode ser fortemente influenciado por outliers.\n",
    "- Penaliza erros grandes de forma desproporcional, o que pode nem sempre ser desejável.\n",
    "\n",
    "# MAE – Mean Absolute Error [2]\n",
    "O Erro Médio Absoluto calcula a diferença média entre os valores calculados e os valores reais.\n",
    "\n",
    "**Vantagens:**\n",
    "\n",
    "- Interpretabilidade: MAE fornece uma medida direta do erro médio, calculando a diferença absoluta média entre os valores previstos e os valores reais. Por exemplo, um MAE de 5 indica que as previsões, em média, desviam-se em 5 unidades dos valores reais.\n",
    "- Robustez a Outliers: MAE é resiliente a outliers, tratando todos os erros de forma igual. Ao contrário do Erro Quadrático Médio (MSE), que penaliza discrepâncias maiores de forma mais intensa, o MAE mantém estabilidade e confiabilidade quando os conjuntos de dados incluem outliers.\n",
    "- Aplicação Prática: MAE é útil em cenários do mundo real onde os custos de erro são lineares, como previsão de demanda ou estimativa de preços de imóveis. Ele oferece insights sobre os tamanhos médios dos erros, tornando-se uma métrica preferida para avaliar a qualidade da previsão com base no erro absoluto em vez do erro relativo.\n",
    "- Penalidade Linear: A natureza linear do MAE significa que o impacto de cada erro na aprendizagem do modelo é diretamente proporcional à sua magnitude. Isso permite que o modelo minimize os erros médios sem focar desproporcionalmente em discrepâncias maiores, garantindo um desempenho equilibrado em todos os pontos de dados.\n",
    "\n",
    "# MAPE – Mean Absolute Percentage Error [3]\n",
    "- O termo MAPE determina quão melhor é a precisão da nossa previsão.\n",
    "- O melhor valor para MAPE é 0, enquanto um valor mais alto indica que as previsões não são precisas o suficiente. No entanto, o quão grande deve ser um valor MAPE para ser considerado uma previsão ineficiente depende do caso de uso.\n",
    "\n",
    "# MDAE – Median Absolute Error [4]\n",
    "A principal vantagem de usar essa métrica é sua forte resiliência a outliers.\n",
    "\n",
    "[1] `https://www.geeksforgeeks.org/mean-squared-error/`  \n",
    "[2] `https://www.geeksforgeeks.org/how-to-calculate-mean-absolute-error-in-python/`  \n",
    "[3] `https://www.geeksforgeeks.org/how-to-calculate-mape-in-python/`  \n",
    "[4] `https://insidelearningmachines.com/median_absolute_error/` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the same columns are present in the training and testing sets\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "# Function to apply PCA and train the regressor model\n",
    "def apply_pca_and_train_regressor(X_train, X_test, y_train, y_test, n_components):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Train the regressor model (example: LGBMRegressor)\n",
    "    model = LGBMRegressor(random_state=42)\n",
    "    model.fit(X_train_pca, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test_pca)\n",
    "    print(f\"Results for PCA with {n_components} components:\")\n",
    "    print(f\"R-Squared: {r2_score(y_test, y_pred):.2f}\")\n",
    "    print(f\"RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Apply PCA with different components and train the model\n",
    "apply_pca_and_train_regressor(X_train, X_test, y_train, y_test, n_components=0.95)\n",
    "apply_pca_and_train_regressor(X_train, X_test, y_train, y_test, n_components=10)\n",
    "apply_pca_and_train_regressor(X_train, X_test, y_train, y_test, n_components=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy predictor for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esta quantidade de features usecols=['manufacturer_name', 'model_name', 'transmission', 'color', \n",
    "                                        'odometer_value', 'year_produced', 'engine_fuel', 'engine_has_gas', \n",
    "                                        'engine_type', 'engine_capacity', 'body_type', 'has_warranty', \n",
    "                                        'state', 'drivetrain', 'price_usd', 'is_exchangeable']\n",
    "\n",
    "\n",
    "e utilizando o lazy regressor, o melhor modelo para este caso foi o LGBMRegressor, no entanto os valores relativos ao RMSE não foram muito positivos visto que 0.45 é um valor alto.\n",
    "\n",
    "Então o passo seguinte foi pegar neste modelo e testar com diferentes features para averiguar qual é o melhor conjunto de features para este problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression models, good values for evaluation metrics indicate how well the model is performing. Here are some common metrics and their ideal values:\n",
    "\n",
    "**R-squared (R²):**\n",
    "\n",
    "Definition: Proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "Good Value: Closer to 1. An R² value of 1 indicates that the model explains all the variability of the response data around its mean.\n",
    "Adjusted R-squared:\n",
    "\n",
    "Definition: Adjusted version of R² that accounts for the number of predictors in the model.\n",
    "Good Value: Closer to 1. It adjusts for the number of predictors, so it is more reliable for comparing models with different numbers of predictors.\n",
    "\n",
    "**Mean Absolute Error (MAE):**\n",
    "\n",
    "Definition: Average of the absolute errors between predicted and actual values.\n",
    "Good Value: Closer to 0. Lower values indicate better model performance.\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "Definition: Average of the squared errors between predicted and actual values.\n",
    "Good Value: Closer to 0. Lower values indicate better model performance.\n",
    "\n",
    "\n",
    "**Root Mean Squared Error (RMSE):**\n",
    "\n",
    "Definition: Square root of the average of the squared errors between predicted and actual values.\n",
    "Good Value: Closer to 0. Lower values indicate better model performance.\n",
    "Mean Absolute Percentage Error (MAPE):\n",
    "\n",
    "Definition: Average of the absolute percentage errors between predicted and actual values.\n",
    "Good Value: Closer to 0%. Lower values indicate better model performance.\n",
    "\n",
    "**Akaike Information Criterion (AIC):**\n",
    "\n",
    "Definition: Measure of the relative quality of a statistical model for a given set of data.\n",
    "Good Value: Lower values are better. It balances model fit and complexity.\n",
    "\n",
    "**Bayesian Information Criterion (BIC):**\n",
    "\n",
    "Definition: Similar to AIC but with a higher penalty for models with more parameters.\n",
    "Good Value: Lower values are better. It also balances model fit and complexity.\n",
    "In general, the ideal values for these metrics depend on the specific context and the nature of the data. Comparing these metrics across different models can help you choose the best model for your regression task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilizar diferentes pca\n",
    "para classificacao fazer histograma com os diferentes preços de carros e criar classes com base num range de preços por exemplo de 0 a 10000, 10001 a 20000, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
